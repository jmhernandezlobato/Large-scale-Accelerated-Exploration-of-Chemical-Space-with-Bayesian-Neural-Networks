\subsubsection{Maximum entropy sampling}

In the paradigm of probabalistic sampling, the best way to quickly reduce the error in a model is to collect maximally informative molecules. To actively collect the most informative molecules we follow an information-based approach \cite{Mackay1992}. The goal is to maximize the expected reduction in posterior entropy that is produced by adding data to the training set. This implies choosing the data point~$\mathbf{x}$ that maximizes
\begin{align}
\text{H}[\mathbf{w}|\mathcal{D}] - 
\mathbb{E}_{y|\mathbf{x},\mathcal{D}}\text{H}[\mathbf{w}|\mathcal{D}\cup\{\mathbf{x},y\}]\,,\label{eq:acquisition_function}
\end{align}
where~$\text{H}[\cdot]$ is the differential entropy,~$\mathbf{w}$ are the model parameters,~$\mathcal{D}$ is the data collected so far and~$y$ is the unknown target associated with~$\mathbf{x}$. We can rewrite~(\ref{eq:acquisition_function}) by swapping the roles of~$y$ and~$\mathbf{w}$~\cite{houlsby2012collaborative}:
\begin{align}
\text{H}[y | \mathbf{x},\mathcal{D}] - 
\mathbb{E}_{\mathbf{W} | \mathcal{D}}\text{H}[y | \mathbf{w}\mathbf{x}]\,.\label{eq:new_acquisition_function}
\end{align}
Since the last term in~(\ref{eq:new_acquisition_function}) is constant, we select the~$\mathbf{x}$ that maximizes the entropy of the predictive distribution~$p(y| \mathbf{x},\mathcal{D})$. When $p(y| \mathbf{x},\mathcal{D})$ is Gaussian, we select the~$\mathbf{x}$ with highest predictive variance