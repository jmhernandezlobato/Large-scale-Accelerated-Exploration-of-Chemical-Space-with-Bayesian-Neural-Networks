\subsubsection{Maximum entropy sampling}

In the paradigm of probabilistic modeling of data, one can reduce prediction error fast by collecting data for the most informative molecules. For this, we follow the information-based approach described in \citea{MacKay_1992}. The goal is to grow the training set by selecting the each next data point to be the one that will on average shrink the most our uncertainty on the optimal value for the network weights. This is equivalent to maximize the expected reduction in posterior entropy that will be produced by adding new data to the training set. Therefore, we choose the next molecule with feature vector $\mathbf{x}$ that maximizes
\begin{equation}
\text{H}[\mathbf{w}|\mathcal{D}] - 
\mathbb{E}_{y|\mathbf{x},\mathcal{D}}\text{H}[\mathbf{w}|\mathcal{D}\cup\{\mathbf{x},y\}]\,,\label{eq:acquisition_function}
\end{equation}
where $\text{H}[\cdot]$ is the differential entropy,~$\mathbf{w}$ are the network weights,~$\mathcal{D}$ is the data collected so far and~$y$ is the unknown target associated with~$\mathbf{x}$. We can rewrite~(\ref{eq:acquisition_function}) by swapping the roles of~$y$ and~$\mathbf{w}$~\cite{houlsby2012collaborative}:
\begin{equation}
\text{H}[y | \mathbf{x},\mathcal{D}] - 
\mathbb{E}_{\mathbf{W} | \mathcal{D}}\text{H}[y | \mathbf{w}\mathbf{x}]\,.\label{eq:new_acquisition_function}
\end{equation}
Since the last term in~(\ref{eq:new_acquisition_function}) is constant, we select the~$\mathbf{x}$ that maximizes the entropy of the predictive distribution~$p(y| \mathbf{x},\mathcal{D})$. When $p(y| \mathbf{x},\mathcal{D})$ is Gaussian, we select the~$\mathbf{x}$ with highest predictive variance