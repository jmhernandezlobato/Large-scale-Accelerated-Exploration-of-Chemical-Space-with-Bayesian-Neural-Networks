\subsection{Thompson Sampling}

In this section we evaluate the gains produced by Thompson sampling (TS) in a simulated high throughput virtual screening setting. For this, we sequentially sample molecules from a library of candidate molecules given by one of the data sets from Section \ref{sec:data_sets}. After each sampling step, we calculate the 1\% recall, that is, the fraction of the top 1\% of molecules from the original library that are found among the sampled ones. Each sampling step consists in selecting a batch of molecules among those that have not been sampled so far. In the Malaria and One-dose data sets we use batches of size 200. These data sets contain each one about 20,000 molecules. The CEP data set contains by contrast 2 million molecules. In this latter case, we use batches of size 1000. 

We first compare the performance of TS with that of two simple baselines. The first one, \emph{greedy}, is a sampling strategy that only considers exploitation and does not perform any exploration. We implement this approach by selecting molecules according to the average of the probabilistic predictions generated by PBP. That is, the greedy approach ignores any variance in the predictions of the Bayesian neural network and generates batches by just ranking molecules according to the mean of the predictive distribution given by PBP. The second baseline is a Monte Carlo approach in which the batches of molecules are selected uniformly at random. These two baselines are examples of techniques that, as the proposed TS method, can be easily applied in a large scale setting in which the library of candidate molecules contains millions of elements.

In the Malaria and One-dose data sets we average across 50 different realizations of the experiments. By contrast, the CEP data set is 100 times larger. In this latter case we report results for a single realization of the experiment (in a second realization we obtained similar results). 
Figure \ref{fig:thompson_1pc} shows the recall obtained by each method in the data sets from 
Section \ref{sec:data_sets}. TS significantly outperforms the Monte Carlo approach, and also offers increased performance and robustness than the greedy sampling methodology. This shows the importance of building in exploration into the sampling strategy, rather than relying on purely exploitative methods. The greedy approach performs best in the CEP data set. In this case, greedy initially finds better molecules than TS. However, after a while TS overtakes greedy, probably because a promising area of chemical space 
initially discovered by the greedy approach starts to become exhausted. The good results of greedy in the CEP data set are also explained by the search landscapes from Figure \ref{fig:info_landscapes}. The plot for CEP clearly contains a single cluster of interesting molecules, while in One-dose and Malaria the interesting molecules are more spread out.

We now consider the savings in computational time produced by the adaptive design strategies. In the CEP data set TS displays a more than thirty times faster discovery rate than the Monte Carlo search, which is in itself faster than the exhaustive enumeration used in the initial exploration of this data set. Even assuming a discovery rate of 30 times as fast as the initial Clean Energy Project, which we belive to be conservative, 34,000 CPU years would have been saved in exploring this part of chemical space.  

Both the One-Dose and Malaria datasets contain around 20,000 molecules; yet by using Thompson sampling, we can locate 70\% of the highly active molecules in both sets, by sampling only 600 molecules. This represents a huge reduction in the discovery time for new theraputic molecules, not to mention a significant saving in the economic costs associated with synthesiszing and testing these molecules.
