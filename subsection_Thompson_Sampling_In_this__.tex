\subsection{Thompson Sampling}

In this section we evaluate the gains produced by Thompson sampling (TS) in a simulated high throughput virtual screening setting. For this, we sequentially sample molecules from a library of candidate molecules given by one of the data sets from Section \ref{sec:data_sets}. After each sampling step, we calculate the 1\% recall, that is, the fraction of the top 1\% of molecules from the original library that are found among the sampled ones. Each sampling step consists in selecting a batch of molecules among those that have not been sampled so far. In the Malaria and One-dose data sets we use batches of size 200. These data sets contain each one about 20,000 molecules. The CEP data set contains by contrast 2 million molecules. In this latter case, we use batches of size 1000. 

We first compare the performance of TS with that of two simple baselines. The first one, \emph{greedy}, is a sampling strategy that only considers exploitation and does not perform any exploration. We implement this approach by selecting molecules according to the average of the probabilistic predictions generated by PBP. That is, the greedy approach ignores any variance in the predictions of the Bayesian neural network and generates batches by just ranking molecules according to the mean of the predictive distribution given by PBP. The second baseline is a Monte Carlo approach in which the batches of molecules are selected uniformly at random. These two baselines are examples of techniques that, as the proposed TS method, can be easily applied in a large scale setting in which the library of candidate molecules contains millions of elements.

In the Malaria and One-dose data sets we average across 50 different realizations of the experiments. By contrast, the CEP data set is 100 times larger and in this latter case we report results for a single realization of the experiment (in a second realization we obtained similar results). 
Figure \ref{fig:thompson_1pc} shows the recall obtained by each method in the data sets from 
Section \ref{sec:data_sets}. TS significantly outperforms the Monte Carlo approach, and also offers increased performance and robustness than the greedy sampling methodology. This shows the importance of building in exploration into the sampling strategy, rather than relying on purely exploitative methods. The greedy approach performs best in the CEP data set. In this case, greedy initially finds better molecules than TS. However, after a while TS overtakes greedy, probably because a promising area of chemical space 
initially discovered by greedy starts to become exhausted. The good results of greedy in the CEP data set are also explained by the search landscapes from Figure \ref{fig:info_landscapes}. The plot for CEP clearly contains a single cluster of interesting molecules, while in One-dose and Malaria the interesting molecules are more spread out.

The previous results allow us to consider the savings in computational time produced by adaptive design strategies. In the CEP data set, TS requires more than thirty times less samples than the Monte Carlo search, which is comparable to the exhaustive enumeration that was actually used to collect the CEP data. We estimate that, with adaptive design strategies, about 34,000 CPU years would have been saved in exploring the part of chemical space covered by the CEP data set. 

Both the One-Dose and Malaria data sets contain around 20,000 molecules. By using TS, we can locate 70\% of the highly active molecules in both sets, by sampling only 600 molecules. This represents a huge reduction in the discovery time for new therapeutic molecules, not to mention a significant saving in the economic costs associated with synthesizing and testing these molecules.

The greedy baseline could be easily modified in the previous experiments to include some amount of exploration. For example, by replacing a small fraction of the molecules in each batch with molecules chosen uniformly at random. This approach is often called $\epsilon$-greedy \cite{watkins1989learning}, where the variable $\epsilon$ indicates the fraction of molecules that are sampled uniformly at random.

\begin{table}
\centering
\caption{Average rank obtained by each method.}\label{tab:results_epsilon_greedy}
\begin{tabular}{lr@{$\pm$}l}
\hline
\bf{Dataset}& \multicolumn{2}{c}{\bf{Rank}}\\
\hline
$\epsilon = 0.01$ & 3.42 & 0.28 \\
$\epsilon = 0.025$ & 3.02 & 0.25 \\
$\epsilon = 0.05$ & 2.86 & 0.23 \\
$\epsilon = 0.075$ & 3.20 & 0.26 \\
TS & \bf{ 2.51 }&\bf{ 0.20 } \\
\hline
\end{tabular}
\end{table}
