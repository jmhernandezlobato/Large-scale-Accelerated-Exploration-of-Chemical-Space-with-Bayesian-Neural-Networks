\subsubsection{Maximum entropy sampling}

To reduce overall prediction error fast, we propose to collect data for the most informative molecules first. For this, we follow the information-based approach described by \cite{MacKay_1992}. The goal is to iteratively add to the training set the data point that will on average shrink the most our uncertainty on the optimal value for the network weights. This is equivalent to maximize the expected reduction in posterior entropy that will be produced by adding a new data point to the training set. In particular, at each step we choose to add the next molecule with feature vector $\mathbf{x}$ that maximizes
\begin{equation}
\text{H}[\mathbf{w}|\mathcal{D}] - 
\mathbb{E}_{y|\mathbf{x},\mathcal{D}}\text{H}[\mathbf{w}|\mathcal{D}\cup\{\mathbf{x},y\}]\,,\label{eq:acquisition_function}
\end{equation}
where $\text{H}[\cdot]$ is the differential entropy,~$\mathbf{w}$ are the network weights,~$\mathcal{D}$ is the data collected so far and~$y$ is the unknown target associated with~$\mathbf{x}$. We can rewrite~(\ref{eq:acquisition_function}) by swapping the roles of~$y$ and~$\mathbf{w}$~\cite{houlsby2012collaborative}:
\begin{equation}
\text{H}[y | \mathbf{x},\mathcal{D}] - 
\mathbb{E}_{\mathbf{W} | \mathcal{D}}\text{H}[y | \mathbf{w}\mathbf{x}]\,.\label{eq:new_acquisition_function}
\end{equation}
Since the last term in~(\ref{eq:new_acquisition_function}) is constant, we select the~$\mathbf{x}$ that maximizes the entropy of the predictive distribution~$p(y| \mathbf{x},\mathcal{D})$. When $p(y| \mathbf{x},\mathcal{D})$ is Gaussian, we select the~$\mathbf{x}$ with highest predictive variance