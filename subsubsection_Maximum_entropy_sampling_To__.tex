\subsubsection{Maximum entropy sampling}

To obtain low prediction error as fast as possible, we propose to collect data for the most informative molecules first. To achieve this, we follow the information-based approach described by \cite{MacKay_1992}. The goal is to iteratively add to the training set the data point that will shrink the most our uncertainty on the optimal value of the network weights. This is equivalent to maximize the expected reduction in posterior entropy. In particular, at each step, we choose to add to the training set the molecule with feature vector $\mathbf{x}$ that maximizes
\begin{equation}
\text{H}[\mathcal{W}|\mathcal{D}]  - 
\mathbb{E}_{y|\mathbf{x},\mathcal{D}}\text{H}[\mathcal{W}|\mathcal{D}\cup\{\mathbf{x},y\}]\,,\label{eq:acquisition_function}
\end{equation}
where $\text{H}[\cdot]$ computes the differential entropy of an input distribution, $\mathcal{W}$ are the network weights, $\mathcal{D}$ is the data collected so far, $y$ is the unknown target associated with $\mathbf{x}$ and $\mathcal{W}|\mathcal{D}$ and $y|\mathbf{x},\mathcal{D}$ stand for $p(\mathcal{W}|\mathcal{D})$ and $p(y|\mathbf{x},\mathcal{D})$, respectively. We can rewrite (\ref{eq:acquisition_function}) by swapping the roles of $y$ and $\mathcal{W}$ to obtain an equivalent expression \cite{houlsby2012collaborative}:
\begin{equation}
\text{H}[y | \mathbf{x},\mathcal{D}] - 
\mathbb{E}_{\mathcal{W} | \mathcal{D}}\text{H}[y | \mathcal{W},\mathbf{x}]\,.\label{eq:new_acquisition_function}
\end{equation}
Note that for fixed $\mathcal{W}$, the predictions of the neural network are Gaussian with variance $\sigma^2$ that is independent of $\mathbf{x}$. This means that the entropy of $p(y| \mathbf{x},\mathcal{W})$ is also independent of $\mathbf{x}$.
Therefore, the last term in (\ref{eq:new_acquisition_function}) is constant.
For this reason, we select the next $\mathbf{x}$ to be the one that maximizes the first term in (\ref{eq:new_acquisition_function}), that is,
the entropy of the predictive distribution $p(y| \mathbf{x},\mathcal{D})$. In PBP, we approximate $p(y| \mathbf{x},\mathcal{D})$ with a Gaussian distribution. Therefore, 
at each step, we add to the training set the molecule with feature vector $\mathbf{x}$ that results in the highest predictive variance.

The previous approach assumes that we are collecting data for one molecule at a time. However, in practice we may perform multiple experiments simultaneously and collect data in batches of molecules. For example, by running computations in parallel in a computer cluster. In this case, if we just select molecules as ranked by the variance of their predictive distribution, we run the risk of including in our batch molecules that are too similar to each other. To avoid this, we can exclude from the current batch any new molecule that is too similar to the ones already included in the batch. For example, by excluding molecules with a Tanimoto distance larger than 0.9 with respect to any of the molecules already included in the batch.

sets of molecules that are very similar to each other.

black-list from a our current batch any molecule that is 


For the CEP dataset, in order to penalize adding sets of molecules which are very similar (and thus contribute less to the entropy) we remove redundent molecules by applying a clustering based upon the Tanimoto distance, with molecules being clustered with a tolerance of 0.9 distance and only one representative of each cluster added to the training set.  The effect of removing redundant molecules is particularly strong in very large datasets such as the CEP, and whilst we tested this method on the other data sets, we found that it had little or no effect upon the outcome, so for these datasets we report the results without the clustering.