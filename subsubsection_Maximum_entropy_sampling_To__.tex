\subsubsection{Maximum entropy sampling}

To obtain low prediction error as fast as possible, we propose to collect data for the most informative molecules first. To achieve this, we follow the information-based approach described by \cite{MacKay_1992}. The goal is to iteratively add to the training set the data point that will shrink the most our uncertainty on the optimal value of the network weights. This is equivalent to maximizing the expected reduction in posterior entropy. In particular, at each step, we choose to add to the training set the molecule with feature vector $\mathbf{x}$ that maximizes
\begin{equation}
\text{H}[\mathcal{W}|\mathcal{D}]  - 
\mathbb{E}_{y|\mathbf{x},\mathcal{D}}\text{H}[\mathcal{W}|\mathcal{D}\cup\{\mathbf{x},y\}]\,,\label{eq:acquisition_function}
\end{equation}
where $\text{H}[\cdot]$ computes the differential entropy of an input distribution, $\mathcal{W}$ are the network weights, $\mathcal{D}$ is the data collected so far, $y$ is the unknown target associated with $\mathbf{x}$ and $\mathcal{W}|\mathcal{D}$ and $y|\mathbf{x},\mathcal{D}$ stand for $p(\mathcal{W}|\mathcal{D})$ and $p(y|\mathbf{x},\mathcal{D})$, respectively. We can rewrite (\ref{eq:acquisition_function}) by swapping the roles of $y$ and $\mathcal{W}$ to obtain an equivalent expression \cite{houlsby2012collaborative}:
\begin{equation}
\text{H}[y | \mathbf{x},\mathcal{D}] - 
\mathbb{E}_{\mathcal{W} | \mathcal{D}}\text{H}[y | \mathcal{W},\mathbf{x}]\,.\label{eq:new_acquisition_function}
\end{equation}
Note that for fixed $\mathcal{W}$, the predictions of the neural network are Gaussian with variance $\sigma^2$ that is independent of $\mathbf{x}$. This means that the entropy of $p(y| \mathbf{x},\mathcal{W})$ is also independent of $\mathbf{x}$.
Therefore, the last term in (\ref{eq:new_acquisition_function}) is constant.
For this reason, we select the next $\mathbf{x}$ to be the one that maximizes the first term in (\ref{eq:new_acquisition_function}), that is,
the entropy of the predictive distribution $p(y| \mathbf{x},\mathcal{D})$. In PBP, we approximate $p(y| \mathbf{x},\mathcal{D})$ with a Gaussian distribution. Therefore, 
at each step, we add to the training set the molecule with feature vector $\mathbf{x}$ that results in the highest predictive variance.

The previous approach assumes that we collect data one molecule at a time. However, in practice, we may
collect data in batches of multiple molecules by performing several experiments simultaneously. For example, by running computations in parallel in a computer cluster. In this case, if we just select molecules as ranked by the variance of their predictive distribution, we run the risk of including in our batch molecules that are too similar to each other, which is undesirable. To avoid this, we can exclude from the current batch any new molecule that is too similar to the ones already included. For example, if the finguerprint-based Tanimoto distance with respect to any of the already included molecules is larger than 0.9. This is a simple but scalable approach that can be applied with large batch sizes. 

In practice, we found that the previous filtering step is only necessary if the library of candidate molecules includes many similar elements such as in the CEP data set described below. We tested this approach on the other analyzed data sets and found that it had little or no effect upon the outcome, so for these other data sets we report results without the described filtering approach.