\subsubsection{Maximum entropy sampling}

In the paradigm of probabalistic sampling, the best way to quickly reduce the error in a model is to collect maximally informative molecules. To actively collect the most informative molecules we follow an information-based approach \cite{MacKay_1992}. The goal
is to select the next data point so that it will on average shrink the most our current uncertainty on the optimal value for the network weights. This is equivalent to maximize the expected reduction in posterior entropy that is produced by adding data to the training set. This implies choosing the next data point $\mathbf{x}$ that maximizes
%\begin{equation}
%\text{H}[\mathbf{w}|\mathcal{D}] - 
%\mathbb{E}_{y|\mathbf{x},\mathcal{D}}\text{H}[\mathbf{w}|\mathcal{D}\cup\{\mathbf{x},y\}]\,,\label{eq:acq_func}
%\end{equation}
