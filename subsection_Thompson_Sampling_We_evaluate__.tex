\subsection{Thompson Sampling}

We evaluate the gains produced by Thompson sampling (TS) in experiments simulating a high throughput virtual screening setting. In these experiments, we sequentially sample molecules from libraries of candidate molecules given by the data sets from Section \ref{sec:data_sets}. After each sampling step, we calculate the 1\% recall, that is, the fraction of the top 1\% of molecules from the original library that are found among the sampled ones. 
For the CEP data set, we compute the recall for molecules with power conversion efficiency larger than 10\%.
Each sampling step consists in selecting a batch of molecules among those that have not been sampled so far. In the Malaria and One-dose data sets we use batches of size 200. These data sets contain each one about 20,000 molecules. By contrast, the CEP data set contains 2 million molecules. In this latter case, we use batches of size 1000. 

We compare the performance of TS with that of two baselines. The first one, \emph{greedy}, is a sampling strategy that only considers exploitation and does not perform any exploration. We implement this approach by selecting molecules according to the average of the probabilistic predictions generated by PBP. That is, the greedy approach ignores any variance in the predictions of the Bayesian neural network and generates batches by just ranking molecules according to the mean of the predictive distribution given by PBP. The second baseline is a Monte Carlo approach in which the batches of molecules are selected uniformly at random. These two baselines are examples of techniques that, as the proposed TS method, can be easily implemented in a large scale setting in which the library of candidate molecules contains millions of elements and data is sampled using large batch sizes.

In the Malaria and One-dose data sets, we average across 50 different realizations of the experiments. This is not possible in the CEP data set, which is 100 times larger than Malaria and One-dose. In the CEP data set we report results for a single realization of the experiment (in a second realization we obtained similar results). 
Figure \ref{fig:thompson_1pc} shows the recall obtained by each method in the data sets from 
Section \ref{sec:data_sets}. TS significantly outperforms the Monte Carlo approach, and also offers increased performance than the greedy sampling methodology. This shows the importance of building in exploration into the sampling strategy, rather than relying on purely exploitative methods. The greedy approach performs best in the CEP data set. In this case, greedy initially finds better molecules than TS. However, after a while TS overtakes greedy, probably because a promising area of chemical space 
initially discovered by greedy starts to become exhausted. The good results of greedy in the CEP data set are also explained by the search landscapes from Figure \ref{fig:info_landscapes}. The plot for CEP clearly contains a single cluster of interesting molecules, while in One-dose and Malaria the interesting molecules are more spread out.

The previous results allow us to consider the savings in computational time produced by adaptive design strategies. In the CEP data set, TS requires more than thirty times less samples than the Monte Carlo search, which is comparable to the exhaustive enumeration that was used to actually collect the CEP data. We estimate that, with adaptive design strategies, about 34,000 CPU years would have been saved in exploring the part of chemical space covered by the CEP data set. Regarding the One-dose and Malaria data sets, both contain around 20,000 molecules. By using TS, we can locate 70\% of the highly active molecules in both sets, by sampling only 600 molecules. This represents a huge reduction in the discovery time for new therapeutic molecules, not to mention savings in the economic costs associated with synthesizing and testing these molecules.

