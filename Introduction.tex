\section{Introduction}

Chemical space is huge, with estimates of over $10^{60}$ molecules. Among
these, less than 100 million compounds are found in public repositories or
databases \cite{Reymond_2012}. This means that many molecules with
useful function for man-kind are still to be discovered (e.g., new energy
materials, pharmaceuticals, dyes, etc.). The vast size of chemical space makes,
however, very challenging to search for new relevant compounds among the many
unimportant ones. Because of this, a discovery process relaying only on
the combination of scientific intuition with trial and error experimentation is slow and tedious.

To accelerate the search, high-throughput approaches can be used in a
combinatorial exploration of small specific areas of chemical space \cite{Rajan_2008}. These have led to the development of
high-throughput virtual screening \cite{Pyzer_Knapp_2015,Halls_2010,Curtarolo_2013,Husch_2015,Subramaniam_2008,Shoichet_2004,Jain_2013}, in which large libraries of molecules are analyzed using
theoretical and computational techniques and then reduced to a small set of
promising leads that experimentalist can follow up on. However, with massive initial libraries, these methods are already
hitting the limits of modern computation---even though they search only a small
drop in the ocean of chemical space. Therefore, at present, there is an urgent
need to accelerate high-throughput screening approaches.

Machine learning (ML) tools can be used for such purpose. These methods can
extrapolate the fitness values of a reduced set of molecules for which data has
already been collected to a much larger set of molecules.  ML techniques can
produce a significant acceleration of the screening process because their
predictions are much faster to compute than the calculations required to
evaluate each molecule's fitness.

Neural networks are ML methods that have been considered by the chemistry
community for solving the aforementioned data prediction problem \cite{Zupan_1991,Burden_1996,Rodemerck_2004,Myint_2012,DuvMacetal15nfp}. As an example, neural networks  were used
recently to predict the efficiency of a large set of organic
photovoltaics generated by combinatorial methods \cite{Pyzer_Knapp_2015a}. In this work, the neural network
predictions were used to discard 99\% of the molecules in the initial
library, saving many expensive computations.

Although the above results are promising, higher speed-ups can in principle be
obtained with adaptive design tools \cite{jones1998efficient}. These techniques
use ML predictions to guide the search and make optimal decisions about what
molecules to analyze next given the data collected so far. The key idea is to
attain a balance between exploration and exploitation in the molecule selection
process. On one hand, we would like to analyze next molecules with high
expected fitness as estimated by the ML method (exploitation). On the other
hand, we would also like to select next molecules for which the ML predictions
are uncertain, since collecting such data is likely to improve the quality of
the predictions in the long run (exploration). For efficient
exploration, it is important to use ML methods that produce accurate
estimates of uncertainty in their predictions. 
Adaptive design tools achieve a balance
between the objectives exploitation and exploration in an iterative loop with
feedback from experiments. 
The result is a general, principled and robust
approach for accelerating the discovery process. 
Figure ? shows the iterative
feedback loop used by adaptive design techniques.

Several studies have applied adaptive design tools to the search for
molecules with improved properties \cite{Xue_2016,Negoescu_2011,De_Grave_2008}. However, these
works are hampered by several limitations. First, the proposed methods are only
applied in the small-data regime, with measurements collected for at most one
thousand molecules---while current high-throughput studies can collect data for
millions of molecules. The reason for using small amounts of data is that these works
use Gaussian processes (GPs) \cite{rasmussen2006gaussian} for making predictions. GPs
produce very accurate estimates of uncertainty in their predictions and are therefore 
good at exploration during the data collection process.
However, they also have a high computational
cost. While GPs can be scaled up by resorting to approximations based on inducing inputs 
\cite{snelson2005sparse,hensman2015scalable}, the resulting methods are
statistically inefficient in the large-data regime because they cannot learn
feature representations for the data \cite{bengio2007scaling}.
Neural networks do not have these disadvantages. They are highly scalable and,
by using adaptive basis functions, they can make very accurate predictions with
large amounts of data \cite{lecun2015deep}. Until very recently, it was very
challenging to obtain accurate estimates of uncertainty in the predictions of very large
neural networks trained on very large datasets. However, in this work we exploit
recent advances in approximate inference with Bayesian neural networks to overcome this difficulty. 
In particular, we use the method probabilistic back-propagation \cite{Hernandez-Lobato15b} to obtain fast
and accurate estimates of uncertainty in the predictions of Bayesian neural networks.

When using adaptive design methods in large scale problems, it is often necessary to collect measurements in 
parallel to speed up the whole process, e.g. by running simulations in a computer cluster or by performing
multiple experiments in a laboratory at the same time. Ideally, the batches with the molecules to be analyzed next
should be generated also in parallel to avoid any computational bottlenecks that can significantly slow down the data collection process. To achieve this, we generate our batches of molecules using the method Thompson sampling 
\cite{thompson_likelihood_1933}, which can be easily implemented in a parallel and distributed fashion. 
This allows us to collect large batches of molecules in a computationally efficient manner.

The rest of the paper is organized as follows. We begin with a brief description of Bayesian neural networks and of
the active learning techniques that we have used for intelligently searching chemical space. We then describe the 
three libraries which we have used to illustrated the advantages of using adaptive design methods with Bayesian neural networks.

through an analysis of the diversity measured using a neural network.  We will then  discuss the results of both active learning techniques when applied to these data sets, illustrating the power of searching in an intelligent manner.

This is a test
