\section{Introduction}

Chemical space is huge, with estimates of over $10^{60}$ molecules. Among
these, less than 100 million compounds are found in public repositories or
databases \cite{Reymond_2012}. This means that many molecules with
useful function for man-kind are still to be discovered (e.g., new energy
materials, pharmaceuticals, dyes, etc.). The vast size of chemical space makes,
however, very challenging to search for new relevant compounds among the many
unimportant ones. Because of this, a discovery process relaying only on
the combination of scientific intuition with trial and error experimentation is slow and tedious.

To accelerate the search, high-throughput approaches can be used in a
combinatorial exploration of small specific areas of chemical space \cite{Rajan_2008}
\cite{rajan_combinatorial_2008}. These have led to the development of
high-throughput virtual screening \cite{Pyzer_Knapp_2015,Halls_2010,Curtarolo_2013,Husch_2015,Subramaniam_2008,Shoichet_2004,Jain_2013}, in which large libraries of molecules are analyzed using
theoretical and computational techniques and then reduced to a small set of
promising leads that experimentalist can follow up on. With masive initial libraries, these methods are already
hitting the limits of modern computation---even though they search only a small
drop in the ocean of chemical space. Therefore, at present, there is an urgent
need to accelerate high-throughput screening approaches.

Machine learning (ML) tools can be used for such purpose. These methods can
extrapolate the fitness values of a reduced set of molecules for which data has
already been collected to a much larger set of molecules.  ML techniques can
produce a significant acceleration of the screening process because their
predictions are much faster to compute than the calculations required to
evaluate each molecule's fitness.

Neural networks are ML methods that have been considered by the chemistry
community for solving the aforementioned data prediction problem
\cite{zupan_neural_1991, burden_using_1996,
rodemerck_application_2004,myint_molecular_2012,DuvMacetal15nfp}. As an example, neural networks  were used
recently to predict the efficiency of a large set of organic
photovoltaics generated by combinatorial methods
\cite{pyzer-knapp_learning_2015-1}. In this work, the neural network
predictions were used to discard 99\% of the molecules in the initial
library, saving many expensive computations.

Although the above results are promising, higher speed-ups can in principle be
obtained with adaptive design tools \cite{jones1998efficient}. These techniques
use ML predictions to guide the search and make optimal decisions about what
molecules to analyze next given the data collected so far. The key idea is to
attain a balance between exploration and explotation in the molecule selection
process. On one hand, we would like to analyze next molecules with high
expected fitness as estimated by the ML method (exploitation). On the other
hand, we would also like to select next molecules for which the ML predicctions
are uncertain, since collecting such data is likely to improve the quality of
the predictions in the long run (exploration). For efficient
exploration, it is important to use ML methods that produce accurate
estimates of uncertainty in their predictions. 
Adaptive design tools achieve a balance
between the objectives explotation and exploration in an iterative loop with
feedback from experiments. 
The result is a general, principled and robust
approach for accelerating the discovery process. 
Figure ? shows the iterative
feedback loop used by adaptive design techniques.

Several studies have already applied adaptive design tools to the search for
molecules with improved properties
\cite{xue2016accelerated,negoescu2011knowledge,de2008active}. However, these
works are hampered by several limitations. First, the proposed methods are only
applied in the small-data regime, with measurements collected for at most one
thousand molecules---while current high-throughput studies can collect data for
millions of molecules. The reason for using small amounts of data is that these
adaptive design methods relay on the predictions generated by Gaussian
processes (GPs) \cite{rasmussen2006gaussian}, which are ML techniques that
produce accurate estimates of uncertainty but also have a high computational
cost. GPs can be scaled up by using approximations based on inducing inputs
\cite{snelson2005sparse,hensman2015scalable}. However, these methods become
statistically inefficient in the large-data regime because they cannot learn
feature representations for the data \cite{bengio2007scaling}.
Neural networks do not have these disadvantages. They are highly scalable and
by using adaptive basis functions, they can make very accurate predictions with
large amounts of data \cite{lecun2015deep}. However, it is in general very