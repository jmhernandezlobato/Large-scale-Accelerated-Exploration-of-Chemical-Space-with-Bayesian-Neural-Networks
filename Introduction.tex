\section{Introduction}

Chemical space is huge, with estimates of over $10^{60}$ molecules. Among
these, less than 100 million compounds are found in public repositories or
databases \cite{Reymond_2012}. This means that many molecules with
useful function for humanity are still to be discovered (e.g., new energy
materials, pharmaceuticals, dyes, etc.). The vast size of chemical space makes,
however, very challenging to search for new relevant compounds among the many
unimportant ones. Because of this, a discovery process relaying only on
the combination of scientific intuition with trial and error experimentation is slow, tedious and in many cases infeasible.

To accelerate the search, high-throughput approaches can be used in a
combinatorial exploration of small specific areas of chemical space \cite{Rajan_2008}. These have led to the development of
high-throughput virtual screening \cite{Pyzer_Knapp_2015,Halls_2010,Curtarolo_2013,Husch_2015,Subramaniam_2008,Shoichet_2004,Jain_2013}, in which large libraries of molecules are analyzed using
theoretical and computational techniques and then reduced to a small set of
promising leads that experimentalist can follow up on. However, with massive initial libraries, these methods are already
hitting the limits of modern computation---even though they search only a small
drop in the ocean of chemical space. Therefore, at present, there is an urgent
need to accelerate high-throughput screening approaches.

Machine learning (ML) tools can be used for such purpose. These methods can
extrapolate the fitness values of a reduced set of molecules for which data has
already been collected to a much larger set of molecules.  ML techniques can
produce a significant acceleration of the screening process because their
predictions are much faster to compute than the calculations required to
evaluate each molecule's fitness.

Neural networks are ML methods that have been considered by the chemistry
community for solving the aforementioned data prediction problem \cite{Zupan_1991,Burden_1996,Rodemerck_2004,Myint_2012,DuvMacetal15nfp}. As an example, neural networks  were used
recently to predict the efficiency of a large set of organic
photovoltaics generated by combinatorial methods \cite{Pyzer_Knapp_2015a}. In this work, the neural network
predictions were used to discard 99\% of the molecules in the initial
library, saving many expensive computations.

Although the above results are promising, higher speed-ups can in principle be
obtained with adaptive design tools \cite{jones1998efficient}. These techniques
use ML predictions to guide the search and make optimal decisions about what
molecules to analyze next given the data collected so far. The key idea is to
attain a balance between exploration and exploitation in the molecule selection
process. On one hand, we would like to analyze next molecules with high
expected fitness as estimated by the ML method (exploitation). On the other
hand, we would also like to select next molecules for which the ML predictions
are uncertain, since collecting such data is likely to improve the quality of
the predictions in the long run (exploration). For efficient
exploration, it is important to use ML methods that produce accurate
estimates of uncertainty in their predictions. 
Adaptive design tools achieve a balance
between the exploitation and exploration objectives in an iterative loop with
feedback from experiments. 
The result is a general, principled and robust
approach for accelerating the discovery process. 
Figure ? shows the iterative
feedback loop used by adaptive design techniques.

Several studies have applied adaptive design tools to searching for
molecules with improved properties \cite{Xue_2016,Negoescu_2011,De_Grave_2008}. 
However, these works only address the small-data regime, with measurements
collected only for at most one thousand molecules. By contrast, current high-throughput studies can collect data for
millions of molecules \cite{Hachmann_2011}. 
The reason for the previous works focusing only
on small data sets is that they 
use Gaussian processes (GPs) \cite{rasmussen2006gaussian} for making predictions. GPs
produce very accurate estimates of uncertainty and are therefore 
good at exploration. However, they also have a high computational cost and can only be applied to small datasets. While GPs can be scaled up by resorting to approximations  \cite{snelson2005sparse,hensman2015scalable}, the resulting methods are
statistically inefficient in the large-data regime because GPs cannot learn
feature representations for the data \cite{bengio2007scaling}.
Neural networks do not have the previous disadvantages: they are highly scalable and,
by using adaptive basis functions, they can make very accurate predictions with
large amounts of data \cite{lecun2015deep}. However, until very recently, it was very
challenging to obtain accurate estimates of uncertainty with large
neural networks and large amounts of data. In this work we exploit
recent advances in approximate inference with Bayesian neural networks to overcome this difficulty. 
In particular, we use the method probabilistic backpropagation \cite{Hernandez-Lobato15b} to obtain fast
and accurate estimates of uncertainty in the predictions of Bayesian neural networks.

As described above, the discovery of new relevant compounds can require to collect massive amounts of data. In this case,
a fully sequential data collection process can be prohibitively slow. A solution is to accelerate the process by using parallelization. For example, by running parallel simulations in a computer cluster or by performing simultaneous experiments in a laboratory.
This requires the adaptive design method to produce at each iteration a batch of molecules that will be
analyzed simultaneously. Ideally, the individual batches of molecules should also be generated in parallel to avoid computational bottlenecks. To achieve this, we generate our batches of molecules using the Thompson sampling heuristic \cite{Thompson_1933},
which can be easily implemented in a parallel and distributed fashion. This allows us to collect large
batches of molecules in a computationally efficient manner. While Thompson sampling allows us to speed up
the search for molecules with optimal properties, we may also be interested in collecting data to quickly
improve our predictions on arbitrary new molecules. In this latter case, we generate our batches
using the maximum entropy sampling heuristic \cite{MacKay_1992}.

The rest of the paper is organized as follows. We begin with a brief description of Bayesian neural networks. We then introduce the probabilistic active learning techniques that we use for sequential adaptive design: Thompson sampling and maximum entropy sampling. After this, we describe three libraries of compounds that we use to illustrate the advantages of our adaptive design methods. Finally, we show the experimental results obtained and the conclusions of this work.
