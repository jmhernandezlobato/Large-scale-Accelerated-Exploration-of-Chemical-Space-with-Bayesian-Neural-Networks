\section{Introduction}

Chemical space is huge: it is estimated to contain over $10^{60}$ molecules. Among these, fewer than 100 million compounds can be found in public repositories or databases \cite{Reymond_2012}. This discrepancy between \textit{known} compounds and \textit{possible} compounds implies that many molecules with useful medical or engineering functions are still to be discovered (e.g., new energy materials, pharmaceuticals, dyes, etc.). While the vast size of chemical space makes this an enormous opportunity, it also presents a significant challenge to the identification of new relevant compounds among the many unimportant ones.  This challenge is so great that any discovery process relying purely on the combination of scientific intuition with trial and error experimentation is slow, tedious, and in many cases infeasible.

To accelerate the search the search for valuable new compounds, high-throughput approaches can be used in a combinatorial exploration of small specific areas of chemical space \cite{Rajan_2008}. These have led to the development of high-throughput virtual screening \cite{Pyzer_Knapp_2015,Halls_2010,Curtarolo_2013,Husch_2015,Subramaniam_2008,Shoichet_2004,Jain_2013,G_mez_Bombarelli_2016}, in which large libraries of molecules are analyzed using theoretical and computational techniques before being culled to a small set of promising compounds for experimentalists to evaluate. However, even searching a tiny drop in the ocean of chemical space can result in a massive initial library of compounds that hits the limits of modern computational capabilities. There is therefore an urgent need to accelerate high-throughput screening approaches.

Machine learning (ML) tools can be used to accelerate screening beyond na\"{i}ve combinatorial and brute-force approaches. These methods can extrapolate the fitness values from a reduced set of previously-evaluated molecules to a much larger set of novel candidates.  ML techniques can, in principle, produce a significant acceleration of the screening process by taking advantage of fast prediction techniques to assess a molecules potential utility.

Neural networks are a class of nonlinear machine learning models that have been considered by the chemistry community for solving the aforementioned data prediction problem \cite{Zupan_1991,Burden_1996,Rodemerck_2004,Myint_2012,DuvMacetal15nfp}. As an example, neural networks  were used
recently to predict the efficiency of a large set of organic photovoltaics generated by combinatorial methods \cite{Pyzer_Knapp_2015a}. In this work, the neural network predictions were used to discard 99\% of the molecules in the initial library, saving many expensive computations.

Although these previous results are promising, higher speed-ups can, in principle, be obtained with adaptive design tools \cite{jones1998efficient}. These techniques use machine learning models to guide the search and make improved decisions about what molecules to analyze next given the data collected so far. The key idea is to attain a balance between exploration and exploitation in the molecule selection process.  That is, on one hand, we would like to expend resources to evaluate molecules with high expected fitness (exploitation), while on the other hand, we would also like to select molecules which provide significant information to the machine learning model and potentially improve future predictions (exploration). For efficient exploration, it is important to use ML methods that produce accurate estimates of uncertainty in their predictions.  Adaptive design tools achieve a balance between the exploitation and exploration objectives in an iterative loop with feedback from experiments.  These techniques provide a general, principled, and robust approach for accelerating the discovery process.  Figure ? shows the iterative feedback loop used by adaptive design techniques.

Several studies have applied adaptive design tools to molecular screening \cite{Xue_2016,Seko_2015,Negoescu_2011,De_Grave_2008}.  However, these works have only addressed the small-data regime, and have only examined at most one thousand molecules. By contrast, current high-throughput studies can collect data for millions of molecules \cite{Hachmann_2011}.  The primary limitation of this previous work has been the use of Gaussian processes (GPs) (see, e.g., \cite{rasmussen2006gaussian} for a review) for making predictions. While GPs can produce accurate estimates of uncertainty and are therefore good for exploration, they also have a high computational cost and can only be applied to small datasets.  While recent work has introduced approximations to improve the scaling of GPs \cite{snelson2005sparse,hensman2015scalable}, the resulting methods are
nevertheless data-inefficient in the large-data regime because GPs cannot learn feature representations for the data \cite{bengio2007scaling}. Neural networks can overcome these disadvantages: they are highly scalable and, by using adaptive basis functions, they can make accurate predictions with large amounts of data \cite{lecun2015deep}. Until recently, however, it was challenging to obtain accurate estimates of uncertainty with large neural networks and large amounts of data. In this work we exploit recent advances in approximate inference with Bayesian neural networks to overcome this difficulty. In particular, we use the method of probabilistic backpropagation \cite{Hernandez-Lobato15b} to obtain fast and accurate estimates of uncertainty when using Bayesian neural networks for prediction.

As described above, the discovery of valuable new compounds can require the collection of massive amounts of data, but a sequential collection process can be prohibitively slow.  One solution is to accelerate the data collection process by using parallelization. For example, by running parallel simulations in a computer cluster or by performing simultaneous experiments in a laboratory. This requires any adaptive design method to produce at each iteration a batch of molecules that can be fruitfully analyzed simultaneously. Ideally, the individual batches of molecules should also be generated in parallel to avoid computational bottlenecks. To achieve this parallelism, we generate our batches of molecules using the Thompson sampling heuristic \cite{Thompson_1933}, which can be easily implemented in a parallel and distributed fashion. While Thompson sampling allows us to speed up the search for molecules with desirable properties, we may also be interested in collecting data to quickly improve our predictions on arbitrary new molecules. In this latter case, we generate our batches using the maximum entropy sampling heuristic \cite{MacKay_1992}.

The rest of the paper is organized as follows. We begin with a brief description of Bayesian neural networks. We then introduce the probabilistic active learning techniques that we use for sequential adaptive design: Thompson sampling and maximum entropy sampling. After this, we describe three libraries of compounds that we use to illustrate the advantages of our adaptive design methods. Finally, we show the experimental results obtained and the conclusions of this work.
