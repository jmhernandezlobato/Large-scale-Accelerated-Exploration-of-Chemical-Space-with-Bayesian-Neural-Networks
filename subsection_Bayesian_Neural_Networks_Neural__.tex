\subsection{Bayesian Neural Networks}

Neural networks are well-suited for our purposes. They produce state-of-the-art results when predicting chemical properties of molecules \cite{Ma_2015,Mayr_2016,ramsundar2015massively} and can be applied to large data sets by using stochastic optimization techniques \cite{bousquet2008tradeoffs}. However, common applications of neural networks focus only on the deterministic prediction scenario. In our case, for successful exploration, it is desirable to use a probabilistic prediction paradigm. While making probabilistic predictions with neural networks is traditionally a difficult task, we use here a recent breakthrough in scalable training of Bayesian neural networks known as probabilistic back-propagation (PBP) \cite{hernandez2015probabilistic}. 

Given data ${\mathcal{D} = \{\mathbf{x}_n, y_n \}_{n=1}^N}$, formed by feature vectors $\mathbf{x}_n \in \mathbb{R}^d$ and targets ${y_n \in \mathbb{R}}$, we assume that $y_n = f(\mathbf{x}_n;\mathcal{W}) + \epsilon_n$, where $f(\cdot ;\mathcal{W})$ is the output of a neural network with weights $\mathcal{W}$. The network output is corrupted by additive noise variables $\epsilon_n \sim \mathcal{N}(0,\gamma^{-1})$. The network has~$L$ layers, with $V_l$ hidden units in layer $l$, and $\mathcal{W} = \{ \mathbf{W}_l \}_{l=1}^L$ is the collection of $V_l \times (V_{l-1}+1)$ weight matrices. The $+1$ is introduced here to account for the additional per-layer biases. The activation functions for the hidden layers are rectifiers: $\varphi(x) = \max(x,0)$. The entries of the matrices in $\mathcal{W}$ follow \emph{a priori} a Gaussian distribution with zero mean and variance $\lambda^{-1}$. The priors on the precision parameters $\gamma$ and $\lambda$ are non-informative Gamma distributions with small shape and rate parameters.

PBP approximates the posterior distribution on $\mathcal{W}$, $\gamma$ and $\lambda$ with the tractable distribution
\begin{equation}
q(\mathcal{W},\gamma, \lambda) = \left[ \prod_{l=1}^L\! \prod_{i=1}^{V_l}\! 
\prod_{j=1}^{V_{l\!-\!1}\!+\!1} \mathcal{N}(w_{ij,l}| m_{ij,l},v_{ij,l})\right ]
 \text{Gama}(\gamma \,|\, \alpha^\gamma, \beta^\gamma)
\text{Gama}(\lambda \,|\, \alpha^\lambda, \beta^\lambda)\,.\label{eq:posterior_approximation}
\end{equation}
whose parameters are tuned by iteratively running an assumed density filtering (ADF) algorithm over the training data \cite{Opper1998}. The main operation in PBP is the update of the mean and variance parameters of (\ref{eq:posterior_approximation})
each time a new data point is processed. For this, PBP matches moments between the new $q$ and the product of the old $q$ and the likelihood factor $\mathcal{N}(y_n \,|\, f(\mathbf{x}_n;\mathcal{W}),\gamma^{-1})$ for the new data point. The matching of moments for the weights is achieved by using standard Gaussian ADF updates (see \cite{minka2001family}, equations 5.12 and 5.1):
\begin{equation}
\label{eq:adf_update_1} m_{ij,l}^\text{new} =  m_{ij,l} + v_{ij,l} \nabla_{m_{ij,l}} \log Z \,,
\end{equation}
\begin{equation}
\label{eq:adf_update_2}v_{ij,l}^\text{new} = v_{ij,l} - v_{ij,l}^2 \left[ (\nabla_{m_{ij,l}} \log Z)^2 - 2 \nabla_{v_{ij,l}} \log Z \right]\,,
\end{equation}
where $Z$ is the average of the likelihood $\mathcal{N}(y_n\,|\, f(\mathbf{x}_n;\mathcal{W}),\gamma^{-1})$ with respect to $q$. 

In the above expressions, the computation of $Z$ is intractable. PBP circumvents this problem by
finding a Gaussian approximation to the distribution of the network output $f(\mathbf{x}_n;\mathcal{W})$ when $\mathcal{W} \sim q$.
This is achieved in a forward pass of the data $\mathbf{x}_n$ through the network, when the weights $\mathcal{W}$ are sampled from $q$. In this forward pass each non-Gaussian distribution for the output of each neural unit is approximated with a Gaussian by moment matching. This is illustrated in Figure \ref{fig:pbp}. Note that when $\gamma\sim q$, the expectation of $\mathcal{N}(y_n \,|\, f(\mathbf{x}_n;\mathcal{W}),\gamma^{-1})$ with respect to $\gamma$ produces a Student's $t$ density which does not convolve in closed form with respect to the Gaussian approximation on $f(\mathbf{x}_n;\mathcal{W})$ when $\mathcal{W} \sim q$. Nevertheless, this Student's $t$ density can be as well approximated with a Gaussian by moment matching to obtain a closed form convolution that approximates $Z$. The gradients required in (\ref{eq:adf_update_1}) and (\ref{eq:adf_update_2}) can then be obtained by back-propagation. 

After several ADF iterations over the data by PBP, we can then obtain a Gaussian approximation to $f(\mathbf{x}_\star;\mathcal{W})$ given the test input $\mathbf{x}_\star$ by applying the forward pass process described above. We refer the reader to \cite{hernandez2015probabilistic} for full details on PBP; a graphical depiction of the process of training a Bayesian neural network using PBP is shown in Figure \ref{fig:pbp}.

