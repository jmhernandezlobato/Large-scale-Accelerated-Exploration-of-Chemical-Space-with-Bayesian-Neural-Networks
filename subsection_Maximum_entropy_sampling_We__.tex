\subsection{Maximum entropy sampling}

We evaluate the performance of the proposed maximum entropy sampling (MES) method by following an experimental protocol similar to the one from Section \ref{sec:thompson_sampling}. We first split randomly all the available molecules into an initial training set of size 2000 (Malaria and One-dose) or 25,000 (CEP), a test set with 10\% of the molecules and a pool set with all the other remaining molecules. After that, we iteratively increase the size of the training set by collecting data from the pool set. For this, we use batches of size 200 (Malaria and One-dose) or 500 (CEP). After each sampling step, we fit a Bayesian neural network on the training data with PBP and compute its predictive root-mean-squared-error (RMSE) on the test data. We use as a baseline a Monte Carlo approach that collects data from the initial library of molecules uniformly at random.

Figure \ref{fig:max_entropy} shows the average results obtained on each data set by each method across 50 repetitions of the experiments for the Malaria and One-dose data sets. For the CEP data set, we repeated the experiments 10 times. The resulting plots indicate that the actively collecting molecules using the maximum entropy sampling methodology affords an improvement in the speed of discovery of maximally informative sets of molecules, with the RMSE curve both dropping much faster to a low error, compared to the Monte Carlo sampling.  For the largest of these sets, the CEP Data Set, the trajectory of this sampling is much noisier for the Monte Carlo sampler, than the maximum entropy sampler.  The smoothness of the maximum entropy sampler is important when considering the application of these methods in `live' situations; sampling 

This result is not surprising when the distribution of pairwise distances derived using the network based components is considered. The result is most pronounced for the Malaria dataset and the One-Dose dataset, which show the most skewed distribution.  The One-Dose dataset has the lowest mean of the three sets, which suggests that molecules picked randomly will most likely have a small 'component distance', this suggests that they will be similar in the eyes of the neural network, and thus contribute little information entropy to the training set. Whilst the Malaria dataset has a higher mean, it exhibits a long-tailed distribution, which is again problematic for a Monte Carlo sampling approach.  In comparison to those, pairwise distances within the Clean Energy Project dataset are fairly close to being normally distributed.  This is realized in the results of the sampling, where Monte Carlo - despite displaying significantly more variance in its performance - does not perform as poorly when compared to the maximum entropy sampling methodology.

It is also worthwhile recalling that this method selects molecules without considering their contribution to the overall error of the training set; instead utilizing the predictive uncertainty of the model for selecting which molecules to add to the training set. Thus, it represents a purely information-based approach and clearly demonstrates the power of exploiting the underlying information landscape for intelligently constructing informative libraries.