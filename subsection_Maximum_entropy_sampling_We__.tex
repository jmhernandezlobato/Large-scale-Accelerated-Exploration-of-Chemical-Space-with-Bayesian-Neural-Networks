\subsection{Maximum entropy sampling}

We now evaluate the performance of the maximum entropy sampling (MES) method. For this, we first randomly split all the available molecules into an initial training set of size 2000 (Malaria and One-dose) or 25,000 (CEP), a test set with 10\% of the molecules and a pool set with all the other remaining molecules. After that, we iteratively increase the size of the training set by collecting data from the pool set. For this, we use batches of size 200 (Malaria and One-dose) or 500 (CEP). Before each sampling step, a Bayesian neural network is fit with PBP on the training data and its predictive root-mean-squared-error (RMSE) is computed on the test data, which is kept fixed during all the experiment. The resulting network is then used to collect data using the MES strategy. For comparison, we also include results for a Monte Carlo approach that collects data by sampling molecules from the pool set uniformly at random.

Figure \ref{fig:max_entropy} shows the average results obtained by each method across 50 (Malaria and One-dose) or 10 (CEP) repetitions of the experiments. These plots indicate that the MES method produces lower RMSE values and is therefore able to discover highly informative sets of molecules. Note that, for the CEP data set, the trajectory of the RMSE curve is much noisier for the Monte Carlo sampler than for the MES sampler. This increased robustness seems to be another advantage of the MES approach.