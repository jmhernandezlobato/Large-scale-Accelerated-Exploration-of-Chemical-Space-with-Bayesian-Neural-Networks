\subsection{Bayesian neural networks and probabilistic back-propagation}

Neural networks are well-suited for our purposes. They produce state-of-the-art results when predicting chemical properties of molecules \cite{Ma_2015,Mayr_2016,ramsundar2015massively} and can be applied to large data sets by using stochastic optimization techniques \cite{bousquet2008tradeoffs}. Common applications of neural networks focus on the deterministic prediction scenario. However, in our case, for successful exploration, it is desirable to use a probabilistic approach that can produce estimates of uncertainty. While making probabilistic predictions with neural networks is traditionally a difficult task, we use here a recent breakthrough in scalable training of Bayesian neural networks known as probabilistic back-propagation (PBP) \cite{hernandez2015probabilistic}. 

Given data ${\mathcal{D} = \{\mathbf{x}_n, y_n \}_{n=1}^N}$, formed by feature vectors $\mathbf{x}_n \in \mathbb{R}^d$ and targets ${y_n \in \mathbb{R}}$, we assume that $y_n = f(\mathbf{x}_n;\mathcal{W}) + \epsilon_n$, where $f(\cdot ;\mathcal{W})$ is the output of a neural network with weights $\mathcal{W}$. The network output is corrupted with additive noise variables $\epsilon_n \sim \mathcal{N}(0,\gamma^{-1})$. The network has~$L$ layers, with $V_l$ hidden units in layer $l$, and $\mathcal{W} = \{ \mathbf{W}_l \}_{l=1}^L$ is the collection of $V_l \times (V_{l-1}+1)$ weight matrices. The $+1$ is introduced here to account for the additional per-layer biases. The activation functions for the hidden layers are rectifiers: $\varphi(x) = \max(x,0)$. The entries of the matrices in $\mathcal{W}$ follow \emph{a priori} a Gaussian distribution with zero mean and variance $\lambda^{-1}$. The priors on the precision parameters $\gamma$ and $\lambda$ are non-informative Gamma distributions with small shape and rate parameters.

PBP approximates the intractable posterior on $\mathcal{W}$, $\gamma$ and $\lambda$ with the tractable distribution
\begin{equation}
q(\mathcal{W},\gamma, \lambda) = \left[ \prod_{l=1}^L\! \prod_{i=1}^{V_l}\! 
\prod_{j=1}^{V_{l\!-\!1}\!+\!1} \mathcal{N}(w_{ij,l}| m_{ij,l},v_{ij,l})\right ]
 \text{Gama}(\gamma \,|\, \alpha^\gamma, \beta^\gamma)
\text{Gama}(\lambda \,|\, \alpha^\lambda, \beta^\lambda)\,,\label{eq:posterior_approximation}
\end{equation}
whose parameters are tuned by iteratively running an assumed density filtering (ADF) algorithm over the training data \cite{Opper1998}. The main operation in PBP is the update of the mean and variance parameters of $q$, that is, the $m_{ij,l}$ and $v_{ij,l}$ in
(\ref{eq:posterior_approximation}), after processing each data point $\{\mathbf{x}_n,y_n\}$. For this, PBP matches moments between the new $q$ and the product of the old $q$ with the corresponding likelihood factor $\mathcal{N}(y_n \,|\, f(\mathbf{x}_n;\mathcal{W}),\gamma^{-1})$. The matching of moments for the weights is achieved by using well-known Gaussian ADF updates, see equations 5.12 and 5.1 in \cite{minka2001family}:
\begin{equation}
\label{eq:adf_update_1} m_{ij,l}^\text{new} =  m_{ij,l} + v_{ij,l} \nabla_{m_{ij,l}} \log Z \,,
\end{equation}
\begin{equation}
\label{eq:adf_update_2}v_{ij,l}^\text{new} = v_{ij,l} - v_{ij,l}^2 \left[ (\nabla_{m_{ij,l}} \log Z)^2 - 2 \nabla_{v_{ij,l}} \log Z \right]\,,
\end{equation}
where $Z$ is the average of the likelihood $\mathcal{N}(y_n\,|\, f(\mathbf{x}_n;\mathcal{W}),\gamma^{-1})$ with respect to $q$. 

In practice, the computation of $Z$ in (\ref{eq:adf_update_1}) and (\ref{eq:adf_update_2}) is intractable. PBP circumvents this problem by finding a Gaussian approximation to the distribution of the network output $f(\mathbf{x}_n;\mathcal{W})$ when $\mathcal{W} \sim q$. This is achieved by performing a forward pass of the feature vector $\mathbf{x}_n$ through the network, with the weights $\mathcal{W}$ being randomly sampled from $q$. In this forward pass the non-Gaussian distributions for the outputs of the neural units are approximated with Gaussians by moment matching. This is illustrated in Figure \ref{fig:pbp}. When $\gamma$ is also random and $\gamma\sim q$, the expectation of $\mathcal{N}(y_n \,|\, f(\mathbf{x}_n;\mathcal{W}),\gamma^{-1})$ with respect to $\gamma$ produces a Student's $t$ density which does not convolve in closed form with respect to the aforementioned Gaussian approximation to the distribution of $f(\mathbf{x}_n;\mathcal{W})$ when $\mathcal{W} \sim q$. Nevertheless, this Student's $t$ density can be as well approximated with a Gaussian by moment matching to obtain a closed form Gaussian convolution that approximates $Z$. The resulting approximation to $Z$ can then be differentiated by back-propagation to obtain the gradients required by (\ref{eq:adf_update_1}) and (\ref{eq:adf_update_2}).

After several ADF iterations over the data by PBP, we can then make predictions for the unkonwn target variable $y_\star$ associated with a new feature vector $\mathbf{x}_\star$. For this, we obtain a Gaussian approximation to $f(\mathbf{x}_\star;\mathcal{W})$ when $\mathcal{W}\sim q$ by applying the forward pass process described above. We refer the reader to \cite{hernandez2015probabilistic} for full details on PBP; a graphical depiction of the process of training a Bayesian neural network using PBP is shown in Figure \ref{fig:pbp}.