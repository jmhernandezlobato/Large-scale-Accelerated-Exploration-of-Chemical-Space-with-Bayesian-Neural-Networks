\subsubsection{Thompson sampling}

Thompson sampling (TS) \cite{Thompson_1933} is a simple heuristic for efficiently identifying the optimal element in a set. Solving this problem with a small number of fitness evaluations requires to attain an optimal exploration/exploitation trade-off. TS does this automatically, without having to use additional parameters for specifying the trade-off explicitly and that would themselves require optimization. Another advantage of Thompson sampling is that it can collect batches of data in parallel. In particular TS can be implemented so that the operations performed for the selection of each element in the batch


in parallel. This is 
particularly important when the library of candidate molecules is very large and contains for example millions of molecules.
The following lines describe the implementation of Thompson sampling for the efficient collection of $N$ new measurements in parallel:
\begin{enumerate}
\item A very small number of molecules are selected randomly from the library and ground truth values are calculated.
\item A Bayesian neural network is trained with PBP on the collected data.
\item The model network weights are sampled $N$ times from their posterior distribution. This results in $N$ neural networks with deterministic weights.
\item Each of the $N$ neural networks makes predictions on all the remaining molecules.
\item For each $N$ neural network, the set of top $N$ molecules with highest predicted score is selected.
ground truth values calculated, and added to the training set.
\item Steps 2-5 are repeated
\end{enumerate}
By sampling the weight distributions in the Bayesian neural network, we produce a set of deterministic neural networks, the weights of which vary across the set in a manner directly related to the uncertainty of the probabilistic model in their value.  Thus, a balance in the search for extreme molecules is achieved without directly imposing any external measure of molecular diversity - which is in itself an area of intense study \cite{Maldonado_2006}.