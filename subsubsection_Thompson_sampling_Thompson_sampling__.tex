\subsubsection{Thompson sampling}

Thompson sampling (TS) \cite{Thompson_1933} is a simple heuristic for identifying the optimal element in a set. Solving this problem efficiently, that is, with a small number of evaluations of the fitness function, requires to attain an optimal exploration/exploitation trade-off. TS does this automatically, without having to use additional parameters for specifying the trade-off explicitly, which would themselves require optimization. 
Another advantage of TS is that it can be easily used
in the batch evaluation setting where multiple measurements are collected simultaneously. Furthermore, the process for generating each new batch of molecules can be implemented in a distributed way across different nodes in a computer cluster. This is important to speed up computations, especially when the batch size is very large and the library of candidate molecules contains millions of elements. The following lines describe the implementation of TS for collecting data in parallel using batches with $N$ molecules:
\begin{enumerate}
\item A small number of molecules are selected uniformly at random from the library of candidate molecules and measurements for their ground truth values are obtained.
\item A Bayesian neural network is trained with PBP on the data collected so far.
\item The network weights are sampled $N$ times from the posterior approximation computed by PBP. This results in $N$ deterministic neural networks with known weight values. Each of these networks makes predictions on all the remaining molecules. For each network, we select the set of top $N$ molecules with highest predicted scores. This results in $N$ sets with $N$ molecules each.
\item A new batch with $N$ molecules is generated. For this, we iterate over the $N$ sets with $N$ molecules obtained in the previous step and select from each set the molecule with highest ranking in that set that has not been selected before.
\item Data is collected for each of the $N$ molecules in the batch.
\item Steps 2-5 are repeated
\end{enumerate}

By sampling the network weights in step 3, TS produces $N$ deterministic neural networks whose predictions attain a balance between exploration and exploitation. Exploitation is enforced because each deterministic neural network is an unbiased estimate of Bayes' optimal predictor. Exploration is obtained as a result of the randomness of the sampling process. 
Thus, a balance in the search for optimal molecules is achieved without directly imposing any external measure of molecular diversity, which is in itself an area of intense study \cite{Maldonado_2006}. 
Note that step 4 is a bottleneck when $N$ and the size of the library of candidate molecules are very large.
Nevertheless, this step can be easily implemented in a distributed manner by making predictions with each of
the $N$ deterministic neural networks on a different node in a computer cluster. Finally, the reason for generating $N$ sets with $N$ molecules in step 4 is to guarantee that we obtain a final batch with $N$ unique molecules, even if the $N$ deterministic networks make exactly the same predictions on the set of candidate molecules.