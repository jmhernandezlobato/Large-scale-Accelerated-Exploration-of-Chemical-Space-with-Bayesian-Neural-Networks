\section{Conclusion}

We have proposed to use Bayesian neural networks as a workhorse for building adaptive design tools to accelerate high-throughput screening experiments. Neural networks can be trained on large amounts of data and, by using adaptive basis functions, they can learn feature representations for improved statistical efficiency. These are properties that alternative methods for adaptive design, e.g., Gaussian processes, do not have. Working with large Bayesian neural networks and large amounts of data is traditionally challenging because of the lack of tools for accurate and scalable approximate inference with these models. Here, we avoided this problem by leveraging on a recent algorithmic advance called probabilistic back-propagation.

We have proposed to use Thompson sampling (TS) as a data collection strategy for quickly finding enriched set of molecules with high fitness values. We have described how to use this method in the batch evaluation setting where multiple measurements are collected simultaneously, as is the case in high-throughput screening settings. We have also indicated how each new batch of molecules can be generated in a distributed way in a computer cluster. This is important to speed up computations when the batch size and the library of molecules to be screened are very large. Our experiments show that TS outperforms other alternative baselines with similar scalability properties such as Monte Carlo, greedy and $\epsilon$-greedy approaches in the task of finding fast the best molecules in a library of candidate compounds.

We have also considered the problem of collecting data that leads to low prediction error fast. For solving this problem we have proposed to search for sets of highly informative molecules. This has been achieved by using the maximum entropy sampling (MES) strategy. Our experiments show that MES produces significant gains in predictive performance with respect to a Monte Carlo approach that just samples molecules uniformly at random.

Finally, we have also introduced the search landscape plots as a way to visualize the complexity of the search problem. By inspecting these plots we can determine whether exploitation would have been important in the design process, e.g. if the optimal molecules appear clustered around a single mode, or exploration was necessary for improved performance, e.g. if the optimal molecules are more spread out across different modes.