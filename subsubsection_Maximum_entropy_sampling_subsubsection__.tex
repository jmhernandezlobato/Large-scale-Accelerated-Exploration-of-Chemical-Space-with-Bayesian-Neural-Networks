\subsubsection{Maximum entropy sampling}

In the paradigm of probabilistic sampling, the best way to quickly reduce the error in a model is to collect maximally informative molecules. To actively collect the most informative molecules we follow an information-based approach \cite{MacKay_1992}. The goal
is to select the next data point so that it will on average shrink the most our current uncertainty on the optimal value for the network weights. This is equivalent to maximize the expected reduction in posterior entropy that is produced by adding data to the training set. This implies choosing the next data point $\mathbf{x}$ that maximizes
\begin{equation}
\text{H}[\mathbf{w}|\mathcal{D}] - 
\mathbb{E}_{y|\mathbf{x},\mathcal{D}}\text{H}[\mathbf{w}|\mathcal{D}\cup\{\mathbf{x},y\}]\,,\label{eq:acquisition_function}
\end{equation}
where $\text{H}[\cdot]$ is the differential entropy,~$\mathbf{w}$ are the network weights,~$\mathcal{D}$ is the data collected so far and~$y$ is the unknown target associated with~$\mathbf{x}$. We can rewrite~(\ref{eq:acquisition_function}) by swapping the roles of~$y$ and~$\mathbf{w}$~\cite{houlsby2012collaborative}:
\begin{equation}
\text{H}[y | \mathbf{x},\mathcal{D}] - 
\mathbb{E}_{\mathbf{W} | \mathcal{D}}\text{H}[y | \mathbf{w}\mathbf{x}]\,.\label{eq:new_acquisition_function}
\end{equation}
Since the last term in~(\ref{eq:new_acquisition_function}) is constant, we select the~$\mathbf{x}$ that maximizes the entropy of the predictive distribution~$p(y| \mathbf{x},\mathcal{D})$. When $p(y| \mathbf{x},\mathcal{D})$ is Gaussian, we select the~$\mathbf{x}$ with highest predictive variance