Auto-encoder neural networks can be used to compress data into a few, representative dimensions. In order to properly condition the weights of the neural network to be close to an optimal solution, the weights of each layer are pre-trained by fitting a two-layer generative model with bi-directional connections (left). These two-layer models are then stacked into a mirrored funnel architecture first decreasing in size (the encoder) and then increasing in size (the decoder). Finally, the weights are fine-tuned to minimize reconstruction error by backpropagation (right). \label{fig:stacked_rbms}