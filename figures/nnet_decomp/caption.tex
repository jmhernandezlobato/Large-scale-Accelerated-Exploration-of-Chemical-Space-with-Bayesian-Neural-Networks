Auto-encoder neural networks can be used to compress data into a few, representative dimensions. These networks use a mirrored funnel architecture first decreasing in size (the encoder) and then increasing in size (the decoder) as shown in the right plot. To properly condition the weights of the neural network to be close to an optimal solution, the weights of each layer are first pre-trained by iteratively fitting two-layer generative models with bi-directional connections (left). These two-layer models are then stacked into the mirrored funnel architecture. Finally, the network weights are fine-tuned to minimize the reconstruction error by backpropagation (right). \label{fig:stacked_rbms}

