Auto-encoder neural networks can be used to compress data into two dimensions for visualization. These networks use a mirrored funnel architecture, first decreasing in size (the encoder) and then increasing in size (the decoder) as shown in the plot in the right. To properly condition the weights of the neural network to be close to an optimal solution, the weights of each layer are first pre-trained by iteratively fitting two-layer generative models with bi-directional connections, as shown in the plot in the left. These two-layer models are then stacked into the mirrored funnel architecture. Finally, the network weights are fine-tuned to minimize the reconstruction error by backpropagation. \label{fig:stacked_rbms}

