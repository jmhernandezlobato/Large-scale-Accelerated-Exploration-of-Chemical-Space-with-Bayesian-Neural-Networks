A graphical representation of the probabilistic back propagation (PBP) algorithm. Deterministic inputs are represented as $1$'s and $0$'s at the input layer. These are passed forward through the Bayesian neural network. Because the weights are random the output of each neuron is also random. The main operation in PBP is the approximation of the random values obtained at the output of each neuron with a Gaussian distribution by moment matching. These operations are performed in a forward pass. The resulting Gaussian approximation to the output of the network is then used to estimate the log of the marginal likelihood. The gradients of this quantity are then used to update the parameters of the Gaussian distributions on the network weights.