A graphical representation of the probabilistic back propagation (PBP)
algorithm. Deterministic inputs corresponding to a molecule encoding are represented as $1$'s and $0$'s at the input
layer. These are passed forward through the Bayesian neural network. The network weights are random, as illustrated
by the small Gaussian densities in the network edges. Because of this, the values obtained at the output of the
neurons are also random. With rectifier non-linearities, the marginal distribution of output values at the hidden units is a mixture
of a point mass at zero and a Gaussian truncated at zero. 
The main operation in PBP is the approximation of
these distributions (shown in black in the plot) with Gaussians densities (shown in red) by moment matching. These
operations are performed in a forward pass. The resulting Gaussian
approximation at the output of the network is then used to estimate the
logarithm of the marginal likelihood. The gradients of this quantity are then
used to update the parameters of the Gaussian distributions on the network
weights.
\label{fig:pbp}