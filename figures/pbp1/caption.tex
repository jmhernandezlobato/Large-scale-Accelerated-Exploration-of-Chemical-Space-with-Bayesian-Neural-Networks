A graphical representation of the probabilistic backpropagation (PBP)
algorithm. Deterministic inputs corresponding to a molecule encoding are represented as $1$'s and $0$'s at the input
layer. These are passed forward through the Bayesian neural network. Because the network weights are random, as illustrated
by the small Gaussian densities in the network edges, the values obtained at the output of the
neurons are also random. With rectifier non-linearities, the marginal distribution of output values at the hidden units is a mixture
of a point mass at zero and a Gaussian truncated at zero. 
The main operation in PBP is the approximation of
these non-Gaussian distributions (shown in black) with Gaussians (shown in red) by moment matching. These
operations are performed in a forward pass. The resulting Gaussian
approximation at the output of the network is then used to estimate the
logarithm of the marginal likelihood. The gradients of this quantity are then
used to update the parameters of the Gaussian distributions on the network
weights using (\ref{eq:adf_update_1}) and (\ref{eq:adf_update_2}).
\label{fig:pbp}